{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guon Dataset: Introduction to non-linear Regression using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source\n",
    "\n",
    "https://github.com/rabah-khalek/TF_tutorials\n",
    "\n",
    "## Learning Goals##\n",
    "This notebook will serve as an introduction to the non-linear regression as well as the new extremely powerful TensorFlow library for Machine Learning (ML) from Google. We will also learn how to use the versatile Pandas package for handling data.\n",
    "\n",
    "\n",
    "## Overview##\n",
    "Throughout, we will work with the [Gluon dataset](https://github.com/rabah-khalek/TF_tutorials/tree/master/PseudoData). It computed using the [LHAPDF](https://lhapdf.hepforge.org) open source code, a general purpose C++ and python interpolator, used for evaluating PDFs from discretised data files.\n",
    "\n",
    "Here is the description of the Gluon dataset we will be playing around with for this notebook:\n",
    ">A gluon is an elementary particle that acts as the exchange particle (or gauge boson) for the strong force between quarks. It is analogous to the exchange of photons in the electromagnetic force between two charged particles. \n",
    "\n",
    ">In technical terms, gluons are vector gauge bosons that mediate strong interactions of quarks in quantum chromodynamics (QCD). Gluons themselves carry the color charge of the strong interaction. This is unlike the photon, which mediates the electromagnetic interaction but lacks an electric charge. Gluons therefore participate in the strong interaction in addition to mediating it, making QCD significantly harder to analyze than QED (quantum electrodynamics).\n",
    "\n",
    ">Because of the inherent non-perturbative nature of partons(quarks and gluon in general) which cannot be observed as free particles, parton densities cannot be calculated using perturbative QCD.\n",
    "Parton distribution functions are obtained by fitting observables to experimental data; they cannot be calculated using perturbative QCD.\n",
    "\n",
    "> The parton density function $f_i(x,Q)$ gives the probability of finding in the proton a parton of flavour $i$ (quarks or gluon) carrying a fraction $x$ of the proton momentum with $Q$ being the energy scale of the hard interaction. Cross sections are calculated by convo- luting the parton level cross section with the PDFs. Since QCD does not predict the parton content of the proton, the shapes of the PDFs are determined by a fit to data from experimental observables in various processes, using the DGLAP evolution equation.\n",
    "\n",
    "> This PseudoData is computed from such fit performed by the [NNPDF collaboration](http://nnpdf.mi.infn.it) that determines the structure of the proton using contemporary methods of artificial intelligence. NNPDF determines PDFs using as an unbiased modeling tool Neural Networks, trained using Genetic Algorithms and recently stochastic Gradient descent, and used to construct a Monte Carlo representation of PDFs and their uncertainties: a probability distribution in a space of functions.\n",
    "\n",
    "\n",
    "## Importing the Gluon data set with Pandas\n",
    "\n",
    "The dataset is a total of 1000 gluon PDF predictions computed between $x=[10^{-6},1]$ for $Q=2\\,GeV$.  \n",
    "<b> Exercise:</b> In what follows, use Pandas to import a random 800 x-points and call that the training data and import the rest 200 x-points and call that the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data parsing is done!\n"
     ]
    }
   ],
   "source": [
    "# Importing the Gluon Data set\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "#Commnet the next line on to turn off warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "seed=12\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "# suppress tflow compilation warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Download the SUSY.csv (about 2GB) from UCI ML archive and save it in the same directory as this jupyter notebook\n",
    "# See: https://archive.ics.uci.edu/ml/machine-learning-databases/00279/\n",
    "#filename=\"SUSY.csv\"\n",
    "filename='PseudoData/gluon_NNPDF31_nlo_pch_as_0118.dat' \n",
    "\n",
    "lines_to_skip = 5\n",
    "\n",
    "columns=[\"x\", \"gluon_cv\", \"gluon_sd\"]\n",
    "# Load 800 rows as train data, 200 as test data\n",
    "\n",
    "df = pd.read_csv(filename, \n",
    "                 sep=\"\\s+\", \n",
    "                 skiprows=lines_to_skip, \n",
    "                 usecols=[0,1,2], \n",
    "                 names=columns)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "df_train = df_train.sort_values(\"x\")\n",
    "df_test = df_test.sort_values(\"x\")\n",
    "\n",
    "print(\"Data parsing is done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
