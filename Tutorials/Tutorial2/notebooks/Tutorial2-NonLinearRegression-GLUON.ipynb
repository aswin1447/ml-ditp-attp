{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gluon Dataset: Introduction to non-linear Regression using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source\n",
    "\n",
    "https://github.com/rabah-khalek/TF_tutorials\n",
    "\n",
    "## Learning Goals##\n",
    "This notebook will serve as an introduction to the non-linear regression as well as the new extremely powerful TensorFlow library for Machine Learning (ML) from Google. We will also learn how to use the versatile Pandas package for handling data.\n",
    "\n",
    "\n",
    "## Overview##\n",
    "Throughout, we will work with the [Gluon dataset](https://github.com/rabah-khalek/TF_tutorials/tree/master/PseudoData). It is computed using the [LHAPDF](https://lhapdf.hepforge.org) open source code, a general purpose C++ and python interpolator, used for evaluating PDFs from discretised data files.\n",
    "\n",
    "Here is the description of the Gluon dataset we will be playing around with for this notebook:\n",
    ">A gluon is an elementary particle that acts as the exchange particle (or gauge boson) for the strong force between quarks. It is analogous to the exchange of photons in the electromagnetic force between two charged particles. \n",
    "\n",
    ">In technical terms, gluons are vector gauge bosons that mediate strong interactions of quarks in quantum chromodynamics (QCD). Gluons themselves carry the color charge of the strong interaction. This is unlike the photon, which mediates the electromagnetic interaction but lacks an electric charge. Gluons therefore participate in the strong interaction in addition to mediating it, making QCD significantly harder to analyze than QED (quantum electrodynamics).\n",
    "\n",
    ">Because of the inherent non-perturbative nature of partons(quarks and gluon in general) which cannot be observed as free particles, parton densities cannot be calculated using perturbative QCD.\n",
    "Parton distribution functions are obtained by fitting observables to experimental data; they cannot be calculated using perturbative QCD.\n",
    "\n",
    "> The parton density function $f_i(x,Q)$ gives the probability of finding in the proton a parton of flavour $i$ (quarks or gluon) carrying a fraction $x$ of the proton momentum with $Q$ being the energy scale of the hard interaction. Cross sections are calculated by convo- luting the parton level cross section with the PDFs. Since QCD does not predict the parton content of the proton, the shapes of the PDFs are determined by a fit to data from experimental observables in various processes, using the DGLAP evolution equation.\n",
    "\n",
    "> This PseudoData is computed from such fit performed by the [NNPDF collaboration](http://nnpdf.mi.infn.it) that determines the structure of the proton using contemporary methods of artificial intelligence. NNPDF determines PDFs using as an unbiased modeling tool Neural Networks, trained using Genetic Algorithms and recently stochastic Gradient descent, and used to construct a Monte Carlo representation of PDFs and their uncertainties: a probability distribution in a space of functions.\n",
    "\n",
    "We Will consider the computed pseudodata to be the `truth` that we're trying to `discover`, but what we will fit actually is the `smeared truth` (noise added on top based on the uncertainties given).\n",
    "\n",
    "## Importing the Gluon data set with Pandas\n",
    "\n",
    "The dataset is a total of 1000 gluon PDF predictions computed between $x=[10^{-6},1]$ for $Q=2\\,GeV$.  \n",
    "<b> Exercise:</b> In what follows, use Pandas to import a random 800 x-points and call that the training data and import the rest 200 x-points and call that the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Data parsing is done!\n"
    }
   ],
   "source": [
    "# Importing the Gluon Data set\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "#Commnet the next line on to turn off warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "# suppress tflow compilation warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Download the SUSY.csv (about 2GB) from UCI ML archive and save it in the same directory as this jupyter notebook\n",
    "# See: https://archive.ics.uci.edu/ml/machine-learning-databases/00279/\n",
    "#filename=\"SUSY.csv\"\n",
    "filename1='PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-3.dat' \n",
    "filename2='PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-4.dat' \n",
    "filename3='PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-5.dat' \n",
    "filename4='PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-6.dat' \n",
    "\n",
    "lines_to_skip = 5\n",
    "\n",
    "columns=[\"x\", \"gluon_cv\", \"gluon_sd\"]\n",
    "# Loading data from txt file\n",
    "df = pd.read_csv(filename1, \n",
    "                 sep=\"\\s+\", \n",
    "                 skiprows=lines_to_skip, \n",
    "                 usecols=[0,1,2], \n",
    "                 names=columns)\n",
    "\n",
    "seed=1991924\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "df_train = df_train.sort_values(\"x\")\n",
    "df_test = df_test.sort_values(\"x\")\n",
    "\n",
    "train_inputs=[]\n",
    "for x in df_train['x']:\n",
    "    train_inputs.append([x])\n",
    "\n",
    "test_inputs=[]\n",
    "for x in df_test['x']:\n",
    "    test_inputs.append([x])\n",
    "\n",
    "\n",
    "#Scaling input features to help the minimizer.\n",
    "train_scaler = StandardScaler()\n",
    "train_scaler.fit(train_inputs)\n",
    "test_scaler = StandardScaler()\n",
    "test_scaler.fit(test_inputs)\n",
    "\n",
    "print(\"Data parsing is done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network using TensorFlow##\n",
    "\n",
    "We're building here the Neural network of 1 input, 1 output, and 1 hidden layer in two ways:\n",
    "- User build kind of way\n",
    "- TensorFlow built-in way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif','serif':['Times']})\n",
    "rc('text', usetex=True)\n",
    "import sys\n",
    "\n",
    "def USER_NN(x, weights, biases, keep_prob):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "#a built-in way to define a neural network\n",
    "def TF_NN(inputs, n_neurones, n_outputs=1):\n",
    "\n",
    "    hidden_layer = tf.layers.dense( x, \n",
    "                                    n_neurones, \n",
    "                                    activation=tf.nn.sigmoid,\n",
    "                                    reuse=tf.AUTO_REUSE)\n",
    "\n",
    "    NN = tf.layers.dense(hidden_layer, n_outputs,name='outputs',reuse=tf.AUTO_REUSE)\n",
    "\n",
    "    return NN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you decide to use the USER_NN, you have to specify here the number of inputs, outputs and hidden layer neurones.\n",
    "\n",
    "Try:\n",
    "- different architectures by modifying the n_hidden_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = 20\n",
    "n_input =  1 #df_train[\"x\"].shape[0]\n",
    "n_target =1 #df_train[\"gluon_cv\"].shape[0]\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_target]))}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_target]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try:\n",
    "- different amount of epochs.\n",
    "- different activation functions: relu, sigmoid, tanh (go back to the definition of the NN above)\n",
    "- different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "('Epoch:', '0001', 'Train cost=', '12.291961670')\n('Epoch:', '1001', 'Train cost=', '0.556858101')\n('Epoch:', '2001', 'Train cost=', '0.260288773')\n('Epoch:', '3001', 'Train cost=', '0.235459480')\n('Epoch:', '4001', 'Train cost=', '0.209126034')\n('Epoch:', '5001', 'Train cost=', '0.181872520')\n('Epoch:', '6001', 'Train cost=', '0.154521189')\n('Epoch:', '7001', 'Train cost=', '0.127873411')\n('Epoch:', '8001', 'Train cost=', '0.103196144')\n('Epoch:', '9001', 'Train cost=', '0.082081661')\n('Epoch:', '10001', 'Train cost=', '0.065617566')\n('Epoch:', '11001', 'Train cost=', '0.053706822')\n('Epoch:', '12001', 'Train cost=', '0.045069780')\n('Epoch:', '13001', 'Train cost=', '0.038324716')\n('Epoch:', '14001', 'Train cost=', '0.032769849')\n('Epoch:', '15001', 'Train cost=', '0.028284490')\n('Epoch:', '16001', 'Train cost=', '0.024701431')\n('Epoch:', '17001', 'Train cost=', '0.021574023')\n('Epoch:', '18001', 'Train cost=', '0.018557645')\n('Epoch:', '19001', 'Train cost=', '0.015707332')\n('Epoch:', '20001', 'Train cost=', '0.013134857')\n('Epoch:', '21001', 'Train cost=', '0.010868911')\n('Epoch:', '22001', 'Train cost=', '0.008997484')\n('Epoch:', '23001', 'Train cost=', '0.007539539')\n('Epoch:', '24001', 'Train cost=', '0.006429950')\n('Epoch:', '25001', 'Train cost=', '0.005570543')\n('Epoch:', '26001', 'Train cost=', '0.004863048')\n('Epoch:', '27001', 'Train cost=', '0.004257658')\n('Epoch:', '28001', 'Train cost=', '0.003754687')\n('Epoch:', '29001', 'Train cost=', '0.003356048')\nOptimization Finished!\n('Test cost:', 0.048984118)\n"
    }
   ],
   "source": [
    "training_epochs = 30000\n",
    "display_step = 1000\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, 1])\n",
    "y = tf.placeholder(\"float\", [None, 1])\n",
    "sigma = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "N_train = len(df_train[\"x\"])\n",
    "N_test = len(df_test[\"x\"])\n",
    "\n",
    "#feature_name = \"x\"\n",
    "#normalized_feature = tf.feature_column.numeric_column(\n",
    "#  feature_name,\n",
    "#  normalizer_fn=zscore)\n",
    "\n",
    "predictions = USER_NN(x, weights, biases, keep_prob)\n",
    "#predictions = TF_NN(x, 20, 1)\n",
    "\n",
    "cost = tf.reduce_mean(tf.square((y-predictions)/sigma)) #/sigma\n",
    "\n",
    "eta = 0.0001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=eta).minimize(cost)\n",
    "#optimizer = tf.train.AdagradOptimizer(learning_rate=eta).minimize(cost)\n",
    "\n",
    "train_x = np.array(train_scaler.transform(train_inputs)).reshape(N_train,1)\n",
    "train_y = np.array(df_train[\"gluon_cv\"]).reshape(N_train,1)\n",
    "train_sigma = np.array(df_train[\"gluon_sd\"]).reshape(N_train,1)\n",
    "train_y += np.random.normal(0, train_sigma)\n",
    "\n",
    "test_x = np.array(test_scaler.transform(test_inputs)).reshape(N_test,1)\n",
    "test_y = np.array(df_test[\"gluon_cv\"]).reshape(N_test,1)\n",
    "test_sigma = np.array(df_test[\"gluon_sd\"]).reshape(N_test,1)\n",
    "test_y += np.random.normal(0, test_sigma)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    avg_cost = 0.0\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        _, c = sess.run([optimizer, cost], \n",
    "                        feed_dict={\n",
    "                            x: train_x, \n",
    "                            y: train_y, \n",
    "                            sigma: train_sigma\n",
    "                        })\n",
    "        avg_cost = c/N_train\n",
    "        \n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"Train cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    predictions_values = sess.run(predictions, \n",
    "                        feed_dict={\n",
    "                            x: train_x, \n",
    "                            y: train_y,\n",
    "                            keep_prob: 0.8\n",
    "                        })\n",
    "    correct_prediction = cost/N_test\n",
    "    print(\"Test cost:\", correct_prediction.eval({x: test_x, y: test_y, sigma: test_sigma}))\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting output\n",
    "\n",
    "Try:\n",
    "\n",
    "- repeat the exercice with different x intervals by changing the file name in first cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = np.array(predictions_values.eval(session=tf.Session()))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(df_train[\"x\"],df_train[\"gluon_cv\"],color='darkblue',label='$t_{g}^{train}$', lw=3)\n",
    "ax.plot(df_test[\"x\"],df_test[\"gluon_cv\"],color='darkgreen',label='$t_{g}^{test}$',alpha=0.3)\n",
    "\n",
    "ax.scatter(df_train[\"x\"],train_y, marker=\"o\", color='darkblue',alpha=1.,label='$d_{g}^{train}$')\n",
    "ax.scatter(df_test[\"x\"],test_y,marker=\"o\",  color = 'darkgreen',alpha=1.,label='$d_{g}^{test}$')\n",
    "\n",
    "#ax.errorbar(df_train[\"x\"],train_y,yerr=train_sigma,color='darkblue',label='$d_{g}^{train}$',alpha=0.3)\n",
    "#ax.errorbar(df_test[\"x\"],test_y,yerr=test_sigma,color='darkgreen',label='$d_{g}^{test}$',alpha=0.3)\n",
    "\n",
    "ax.plot(df_train[\"x\"],predictions_values,color='red',label='$NN_{g}$', lw=3)\n",
    "ax.fill_between(df_train[\"x\"],df_train[\"gluon_cv\"]+df_train[\"gluon_sd\"],\n",
    "                df_train[\"gluon_cv\"]-df_train[\"gluon_sd\"] ,color='blue', \n",
    "                alpha=0.3, label='$\\sigma_{g}$')\n",
    "\n",
    "#df_train.plot(kind='line',x='x',y=['gluon_cv'], yerr='gluon_sd',color=['red'], ax=ax)\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel(r'g(x)',fontsize=25)\n",
    "ax.set_xlabel(r'x',fontsize=25)\n",
    "ax.legend(loc='best',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}